{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "abfabd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# core train\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import zarr\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import random\n",
    "from skimage.transform import resize\n",
    "\n",
    "import math\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d2e012ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"data\": {\n",
    "        \"batch_size\": 64,\n",
    "        \"crop_scale\": [0.3, 1.0],\n",
    "        \"crop_size\": 224,\n",
    "        \"image_folders\": [\"ssl-s2l1c/data/ssl4eo-s12/train/S2L1C\",\n",
    "                          \"ssl-s2l2a/data/ssl4eo-s12/train/S2L2A\"],\n",
    "        \"num_workers\": 2,\n",
    "        \"pin_mem\": True,\n",
    "        \"root_path\": \"/kaggle/input\",\n",
    "        \"use_horizontal_flip\": False\n",
    "    },\n",
    "    \"logging\": {\n",
    "        \"folder\": \"/kaggle/working/logs\",\n",
    "        \"write_tag\": \"jepa\"\n",
    "    },\n",
    "    \"mask\": {\n",
    "        \"allow_overlap\": False,\n",
    "        \"aspect_ratio\": [0.75, 1.5],\n",
    "        \"enc_mask_scale\": [0.85, 1.0],\n",
    "        \"min_keep\": 10,\n",
    "        \"num_enc_masks\": 1,\n",
    "        \"num_pred_masks\": 4,\n",
    "        \"patch_size\": 14,\n",
    "        \"pred_mask_scale\": [0.15, 0.2]\n",
    "    },\n",
    "    \"meta\": {\n",
    "        \"copy_data\": False,\n",
    "        \"load_checkpoint\": False,\n",
    "        \"model_name\": \"vit_huge\",\n",
    "        \"pred_depth\": 12,\n",
    "        \"pred_emb_dim\": 384,\n",
    "        \"read_checkpoint\": None,\n",
    "        \"use_bfloat16\": True\n",
    "    },\n",
    "    \"optimization\": {\n",
    "        \"ema\": [0.996, 1.0],\n",
    "        \"epochs\": 2,\n",
    "        \"final_lr\": 1.0e-5,\n",
    "        \"final_weight_decay\": 0.4,\n",
    "        \"ipe_scale\": 1.0,\n",
    "        \"lr\": 0.001,\n",
    "        \"start_lr\": 0.0002,\n",
    "        \"warmup\": 20,\n",
    "        \"weight_decay\": 0.04\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f4acebb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yaml' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[99], line 59\u001b[0m\n\u001b[0;32m     57\u001b[0m dump \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams-ijepa.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(dump, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 59\u001b[0m     \u001b[43myaml\u001b[49m\u001b[38;5;241m.\u001b[39mdump(args, f)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'yaml' is not defined"
     ]
    }
   ],
   "source": [
    "resume_preempt = False\n",
    "rank = 0\n",
    "\n",
    "# -- META\n",
    "use_bfloat16 = args['meta']['use_bfloat16']\n",
    "model_name = args['meta']['model_name']\n",
    "load_model = args['meta']['load_checkpoint'] or resume_preempt\n",
    "r_file = args['meta']['read_checkpoint']\n",
    "copy_data = args['meta']['copy_data']\n",
    "pred_depth = args['meta']['pred_depth']\n",
    "pred_emb_dim = args['meta']['pred_emb_dim']\n",
    "if not torch.cuda.is_available():\n",
    "    device = torch.device('cpu')\n",
    "else:\n",
    "    device = torch.device('cuda:0')\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "# -- DATA\n",
    "use_horizontal_flip = args['data']['use_horizontal_flip']\n",
    "# --\n",
    "batch_size = args['data']['batch_size']\n",
    "pin_mem = args['data']['pin_mem']\n",
    "num_workers = args['data']['num_workers']\n",
    "root_path = args['data']['root_path']\n",
    "image_folders = args['data']['image_folders']\n",
    "crop_size = args['data']['crop_size']\n",
    "crop_scale = args['data']['crop_scale']\n",
    "# --\n",
    "\n",
    "# -- MASK\n",
    "allow_overlap = args['mask']['allow_overlap']  # whether to allow overlap b/w context and target blocks\n",
    "patch_size = args['mask']['patch_size']  # patch-size for model training\n",
    "num_enc_masks = args['mask']['num_enc_masks']  # number of context blocks\n",
    "min_keep = args['mask']['min_keep']  # min number of patches in context block\n",
    "enc_mask_scale = args['mask']['enc_mask_scale']  # scale of context blocks\n",
    "num_pred_masks = args['mask']['num_pred_masks']  # number of target blocks\n",
    "pred_mask_scale = args['mask']['pred_mask_scale']  # scale of target blocks\n",
    "aspect_ratio = args['mask']['aspect_ratio']  # aspect ratio of target blocks\n",
    "# --\n",
    "\n",
    "# -- OPTIMIZATION\n",
    "ema = args['optimization']['ema']\n",
    "ipe_scale = args['optimization']['ipe_scale']  # scheduler scale factor (def: 1.0)\n",
    "wd = float(args['optimization']['weight_decay'])\n",
    "final_wd = float(args['optimization']['final_weight_decay'])\n",
    "num_epochs = args['optimization']['epochs']\n",
    "warmup = args['optimization']['warmup']\n",
    "start_lr = args['optimization']['start_lr']\n",
    "lr = args['optimization']['lr']\n",
    "final_lr = args['optimization']['final_lr']\n",
    "\n",
    "# -- LOGGING\n",
    "folder = args['logging']['folder']\n",
    "tag = args['logging']['write_tag']\n",
    "\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "dump = os.path.join(folder, 'params-ijepa.yaml')\n",
    "with open(dump, 'w') as f:\n",
    "    yaml.dump(args, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4188ee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose:\n",
    "    \"\"\"\n",
    "    Composes several transforms together.\n",
    "    Args:\n",
    "        transforms (list of callables): list of transforms to compose.\n",
    "    \"\"\"\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img_array):\n",
    "        \"\"\"\n",
    "        Applies the composed transforms to the input array.\n",
    "        The input can be a NumPy array. The output will be a torch.Tensor\n",
    "        if torch.from_numpy is the last transform.\n",
    "        Args:\n",
    "            img_array (numpy.ndarray): Input image array (C, H, W).\n",
    "        Returns:\n",
    "            torch.Tensor: Transformed image tensor.\n",
    "        \"\"\"\n",
    "        for t in self.transforms:\n",
    "            img_array = t(img_array) # Note: img_array will become a torch.Tensor at the end\n",
    "        return img_array\n",
    "\n",
    "\n",
    "def scale_to_01_np(img_array, max_int_value=32767.0):\n",
    "    \"\"\"\n",
    "    Scales positive integer values in a NumPy array to the 0-1 range.\n",
    "    Assumes input values are positive and fit within max_int_value.\n",
    "    Args:\n",
    "        img_array (numpy.ndarray): Input image array.\n",
    "        max_int_value (float): The maximum possible integer value in the original array.\n",
    "                               For int16, this is typically 32767.\n",
    "    Returns:\n",
    "        numpy.ndarray: Scaled image array.\n",
    "    \"\"\"\n",
    "    return img_array / max_int_value\n",
    "\n",
    "\n",
    "def ensure_13_channels(img_array):\n",
    "    \"\"\"\n",
    "    Ensures the input array has exactly 13 channels by adding the pixel-wise mean\n",
    "    of the first 12 channels as the 13th channel.\n",
    "\n",
    "    Args:\n",
    "        img_array (numpy.ndarray): Input image array (C, H, W)\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Array with exactly 13 channels\n",
    "    \"\"\"\n",
    "    C, H, W = img_array.shape\n",
    "    img_array = img_array.astype(np.float32)\n",
    "\n",
    "    # If already 13 channels, return as is\n",
    "    if C == 13:\n",
    "        return img_array\n",
    "\n",
    "    # If 12 channels, add pixel-wise mean as 13th channel\n",
    "    elif C == 12:\n",
    "        mean_channel = np.mean(img_array, axis=0, keepdims=True)  # Shape: (1, H, W)\n",
    "        return np.concatenate([img_array, mean_channel], axis=0)\n",
    "\n",
    "    # For other numbers of channels (unexpected case)\n",
    "    else:\n",
    "        raise ValueError(f\"Expected 12 or 13 channels, but got {C}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7d40edc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference_transforms(image_size=224):\n",
    "    \"\"\"Create transforms for inference without random augmentations.\"\"\"\n",
    "    transform_list = [\n",
    "        # Convert input image to 13 channels if needed\n",
    "        ensure_13_channels,  \n",
    "        \n",
    "        # Simple resize instead of random crop - with proper channel handling\n",
    "        lambda x: resize_chw(x, image_size),\n",
    "        \n",
    "        # Scale to 0-1 range\n",
    "        lambda x: scale_to_01_np(x, max_int_value=32767.0),\n",
    "        \n",
    "        # Convert to torch tensor\n",
    "        torch.from_numpy\n",
    "    ]\n",
    "    \n",
    "    # Create transform composition\n",
    "    return Compose(transform_list)\n",
    "\n",
    "def resize_chw(img, image_size):\n",
    "    \"\"\"\n",
    "    Resize an image in CHW format (channels, height, width) to a square size.\n",
    "    \n",
    "    Args:\n",
    "        img: NumPy array in shape (C, H, W)\n",
    "        image_size: Target size (will be a square image)\n",
    "        \n",
    "    Returns:\n",
    "        Resized image in shape (C, image_size, image_size)\n",
    "    \"\"\"\n",
    "    c, h, w = img.shape\n",
    "    resized_img = np.zeros((c, image_size, image_size), dtype=img.dtype)\n",
    "    \n",
    "    # Resize each channel independently\n",
    "    for i in range(c):\n",
    "        # Temporarily reshape to (h, w) for resize\n",
    "        channel = img[i]\n",
    "        # Resize to target shape\n",
    "        resized_channel = resize(channel, (image_size, image_size), \n",
    "                                order=1, mode='reflect', anti_aliasing=True)\n",
    "        # Store back in the result array\n",
    "        resized_img[i] = resized_channel\n",
    "        \n",
    "    return resized_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "20d418ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_masks(x, masks):\n",
    "    \"\"\"\n",
    "    :param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
    "    :param masks: list of tensors containing indices of patches in [N] to keep\n",
    "    \"\"\"\n",
    "    all_x = []\n",
    "    for m in masks:\n",
    "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1))\n",
    "        all_x += [torch.gather(x, dim=1, index=mask_keep)]\n",
    "    return torch.cat(all_x, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5a5c47d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
    "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
    "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
    "    def norm_cdf(x):\n",
    "        # Computes standard normal cumulative distribution function\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Values are generated by using a truncated uniform distribution and\n",
    "        # then using the inverse CDF for the normal distribution.\n",
    "        # Get upper and lower cdf values\n",
    "        l = norm_cdf((a - mean) / std)\n",
    "        u = norm_cdf((b - mean) / std)\n",
    "\n",
    "        # Uniformly fill tensor with values from [l, u], then translate to\n",
    "        # [2l-1, 2u-1].\n",
    "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
    "\n",
    "        # Use inverse cdf transform for normal distribution to get truncated\n",
    "        # standard normal\n",
    "        tensor.erfinv_()\n",
    "\n",
    "        # Transform to proper mean, std\n",
    "        tensor.mul_(std * math.sqrt(2.))\n",
    "        tensor.add_(mean)\n",
    "\n",
    "        # Clamp to ensure it's in the proper range\n",
    "        tensor.clamp_(min=a, max=b)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    # type: (Tensor, float, float, float, float) -> Tensor\n",
    "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
    "\n",
    "\n",
    "def repeat_interleave_batch(x, B, repeat):\n",
    "    N = len(x) // B\n",
    "    x = torch.cat([\n",
    "        torch.cat([x[i*B:(i+1)*B] for _ in range(repeat)], dim=0)\n",
    "        for i in range(N)\n",
    "    ], dim=0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "01c8dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size, dtype=float)\n",
    "    grid_w = np.arange(grid_size, dtype=float)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid length\n",
    "    return:\n",
    "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid = np.arange(grid_size, dtype=float)\n",
    "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=float)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega   # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)   # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out)  # (M, D/2)\n",
    "    emb_cos = np.cos(out)  # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x, attn\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        y, attn = self.attn(self.norm1(x))\n",
    "        if return_attention:\n",
    "            return attn\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    3x3 Convolution stems for ViT following ViTC models\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, strides, img_size=224, in_chans=13, batch_norm=True):\n",
    "        super().__init__()\n",
    "        # Build the stems\n",
    "        stem = []\n",
    "        channels = [in_chans] + channels\n",
    "        for i in range(len(channels) - 2):\n",
    "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3,\n",
    "                               stride=strides[i], padding=1, bias=(not batch_norm))]\n",
    "            if batch_norm:\n",
    "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
    "            stem += [nn.ReLU(inplace=True)]\n",
    "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
    "        self.stem = nn.Sequential(*stem)\n",
    "\n",
    "        # Comptute the number of patches\n",
    "        stride_prod = int(np.prod(strides))\n",
    "        self.num_patches = (img_size[0] // stride_prod)**2\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.stem(x)\n",
    "        return p.flatten(2).transpose(1, 2)\n",
    "\n",
    "\n",
    "class VisionTransformerPredictor(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_patches,\n",
    "        embed_dim=768,\n",
    "        predictor_embed_dim=384,\n",
    "        depth=6,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        init_std=0.02,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        # --\n",
    "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim),\n",
    "                                                requires_grad=False)\n",
    "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1],\n",
    "                                                      int(num_patches**.5),\n",
    "                                                      cls_token=False)\n",
    "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0))\n",
    "        # --\n",
    "        self.predictor_blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=predictor_embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.predictor_norm = norm_layer(predictor_embed_dim)\n",
    "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
    "        # ------\n",
    "        self.init_std = init_std\n",
    "        trunc_normal_(self.mask_token, std=self.init_std)\n",
    "        self.apply(self._init_weights)\n",
    "        self.fix_init_weight()\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.predictor_blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, masks_x, masks):\n",
    "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
    "\n",
    "        if not isinstance(masks_x, list):\n",
    "            masks_x = [masks_x]\n",
    "\n",
    "        if not isinstance(masks, list):\n",
    "            masks = [masks]\n",
    "\n",
    "        # -- Batch Size\n",
    "        B = len(x) // len(masks_x)\n",
    "\n",
    "        # -- map from encoder-dim to pedictor-dim\n",
    "        x = self.predictor_embed(x)\n",
    "\n",
    "        # -- add positional embedding to x tokens\n",
    "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1)\n",
    "        x += apply_masks(x_pos_embed, masks_x)\n",
    "\n",
    "        _, N_ctxt, D = x.shape\n",
    "\n",
    "        # -- concat mask tokens to x\n",
    "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1)\n",
    "        pos_embs = apply_masks(pos_embs, masks)\n",
    "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))\n",
    "        # --\n",
    "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n",
    "        # --\n",
    "        pred_tokens += pos_embs\n",
    "        x = x.repeat(len(masks), 1, 1)\n",
    "        x = torch.cat([x, pred_tokens], dim=1)\n",
    "\n",
    "        # -- fwd prop\n",
    "        for blk in self.predictor_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.predictor_norm(x)\n",
    "\n",
    "        # -- return preds for mask tokens\n",
    "        x = x[:, N_ctxt:]\n",
    "        x = self.predictor_proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=[224],\n",
    "        patch_size=16,\n",
    "        in_chans=13,\n",
    "        embed_dim=768,\n",
    "        predictor_embed_dim=384,\n",
    "        depth=12,\n",
    "        predictor_depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        init_std=0.02,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        # --\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size[0],\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        # --\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
    "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1],\n",
    "                                            int(self.patch_embed.num_patches**.5),\n",
    "                                            cls_token=False)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "        # --\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        # ------\n",
    "        self.init_std = init_std\n",
    "        self.apply(self._init_weights)\n",
    "        self.fix_init_weight()\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, masks=None):\n",
    "        if masks is not None:\n",
    "            if not isinstance(masks, list):\n",
    "                masks = [masks]\n",
    "\n",
    "        # -- patchify x\n",
    "        x = self.patch_embed(x)\n",
    "        B, N, D = x.shape\n",
    "\n",
    "        # -- add positional embedding to x\n",
    "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
    "        x = x + pos_embed\n",
    "\n",
    "        # -- mask x\n",
    "        if masks is not None:\n",
    "            x = apply_masks(x, masks)\n",
    "\n",
    "        # -- fwd prop\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def interpolate_pos_encoding(self, x, pos_embed):\n",
    "        npatch = x.shape[1] - 1\n",
    "        N = pos_embed.shape[1] - 1\n",
    "        if npatch == N:\n",
    "            return pos_embed\n",
    "        class_emb = pos_embed[:, 0]\n",
    "        pos_embed = pos_embed[:, 1:]\n",
    "        dim = x.shape[-1]\n",
    "        pos_embed = nn.functional.interpolate(\n",
    "            pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "            scale_factor=math.sqrt(npatch / N),\n",
    "            mode='bicubic',\n",
    "        )\n",
    "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
    "\n",
    "\n",
    "def vit_predictor(**kwargs):\n",
    "    model = VisionTransformerPredictor(\n",
    "        mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "        **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_tiny(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_small(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_base(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_large(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_huge(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_giant(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=1408, depth=40, num_heads=16, mlp_ratio=48/11,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "VIT_EMBED_DIMS = {\n",
    "    'vit_tiny': 192,\n",
    "    'vit_small': 384,\n",
    "    'vit_base': 768,\n",
    "    'vit_large': 1024,\n",
    "    'vit_huge': 1280,\n",
    "    'vit_giant': 1408,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d78d579",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5a61a551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(\n",
    "    device,\n",
    "    r_path,\n",
    "    encoder,\n",
    "    predictor,\n",
    "    target_encoder,\n",
    "    opt,\n",
    "    scaler,\n",
    "):\n",
    "    try:\n",
    "        checkpoint = torch.load(r_path, map_location=torch.device('cpu'))\n",
    "        epoch = checkpoint['epoch']\n",
    "\n",
    "        # -- loading encoder\n",
    "        pretrained_dict = checkpoint['encoder']\n",
    "        msg = encoder.load_state_dict(pretrained_dict)\n",
    "\n",
    "        # -- loading predictor\n",
    "        pretrained_dict = checkpoint['predictor']\n",
    "        msg = predictor.load_state_dict(pretrained_dict)\n",
    "\n",
    "        # -- loading target_encoder\n",
    "        if target_encoder is not None:\n",
    "            print(list(checkpoint.keys()))\n",
    "            pretrained_dict = checkpoint['target_encoder']\n",
    "            msg = target_encoder.load_state_dict(pretrained_dict)\n",
    "\n",
    "        # -- loading optimizer\n",
    "        opt.load_state_dict(checkpoint['opt'])\n",
    "        if scaler is not None:\n",
    "            scaler.load_state_dict(checkpoint['scaler'])\n",
    "        del checkpoint\n",
    "\n",
    "    except Exception as e:\n",
    "        epoch = 0\n",
    "\n",
    "    return encoder, predictor, target_encoder, opt, scaler, epoch\n",
    "\n",
    "\n",
    "def init_model(\n",
    "    device,\n",
    "    patch_size=16,\n",
    "    model_name='vit_base',\n",
    "    crop_size=224,\n",
    "    pred_depth=6,\n",
    "    pred_emb_dim=384\n",
    "):\n",
    "    encoder = vit_small(\n",
    "        img_size=[crop_size],\n",
    "        patch_size=patch_size)\n",
    "    predictor = vit_predictor(\n",
    "        num_patches=encoder.patch_embed.num_patches,\n",
    "        embed_dim=encoder.embed_dim,\n",
    "        predictor_embed_dim=pred_emb_dim,\n",
    "        depth=pred_depth,\n",
    "        num_heads=encoder.num_heads)\n",
    "\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, torch.nn.LayerNorm):\n",
    "            torch.nn.init.constant_(m.bias, 0)\n",
    "            torch.nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    for m in encoder.modules():\n",
    "        init_weights(m)\n",
    "\n",
    "    for m in predictor.modules():\n",
    "        init_weights(m)\n",
    "\n",
    "    encoder.to(device)\n",
    "    predictor.to(device)\n",
    "    return encoder, predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d090c39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IJEPA_Extractor:\n",
    "    def __init__(self, checkpoint_path, stats_path, device='cuda', include_predictor=True):\n",
    "        \"\"\"Load a pre-trained I-JEPA model for feature extraction and prediction visualization.\n",
    "        \n",
    "        Args:\n",
    "            checkpoint_path: Path to the saved model checkpoint\n",
    "            device: Device to run inference on ('cuda' or 'cpu')\n",
    "            include_predictor: Whether to load the predictor for visualization\n",
    "        \"\"\"\n",
    "        self.device = torch.device(device if torch.cuda.is_available() and device=='cuda' else 'cpu')\n",
    "\n",
    "        \n",
    "        # Load stats for normalization\n",
    "        self.stats = torch.load(stats_path, map_location=self.device)\n",
    "        \n",
    "        # Initialize model architecture\n",
    "        self.encoder, self.predictor = init_model(\n",
    "            device=self.device,\n",
    "            patch_size=14,  # Use the same patch_size as during training\n",
    "            crop_size=224,  # Use standard size for inference\n",
    "            model_name='vit_small',  # Use the same architecture as during training\n",
    "            pred_depth=12,  # Use the same predictor depth as during training\n",
    "            pred_emb_dim=384  # Use the same embedding dimension as during training\n",
    "        )\n",
    "        \n",
    "        # Load weights from checkpoint\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "        self.encoder.load_state_dict(checkpoint['encoder'])\n",
    "        self.encoder.eval()\n",
    "        \n",
    "        if include_predictor:\n",
    "            self.predictor.load_state_dict(checkpoint['predictor'])\n",
    "            self.predictor.eval()\n",
    "        else:\n",
    "            self.predictor = None\n",
    "        \n",
    "        # Prepare transforms\n",
    "        self.transforms = get_inference_transforms(224)\n",
    "\n",
    "\n",
    "    def extract_features(self, image):\n",
    "        \"\"\"Extract features from a single image.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image in numpy format (C, H, W)\n",
    "            \n",
    "        Returns:\n",
    "            features: Tensor of extracted features\n",
    "        \"\"\"\n",
    "        # Apply transforms\n",
    "        img = self.transforms(image)\n",
    "        \n",
    "        # Apply normalization\n",
    "        mean = self.stats['mean'].to(self.device).view(-1, 1, 1)\n",
    "        std = self.stats['std'].to(self.device).view(-1, 1, 1)\n",
    "        img = (img - mean) / (std + 1e-8)\n",
    "        \n",
    "        # Add batch dimension and move to device\n",
    "        img = img.unsqueeze(0).to(self.device)\n",
    "        \n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            features = self.encoder(img)\n",
    "            \n",
    "        return features\n",
    "        \n",
    "\n",
    "    def batch_extract_features(self, images):\n",
    "        \"\"\"Extract features from a batch of images.\n",
    "        \n",
    "        Args:\n",
    "            images: List of input images\n",
    "            \n",
    "        Returns:\n",
    "            features: Batch tensor of extracted features\n",
    "        \"\"\"\n",
    "        batch = []\n",
    "        for img in images:\n",
    "            # Apply transforms\n",
    "            img = self.transforms(img)\n",
    "            \n",
    "            # Apply normalization\n",
    "            mean = self.stats['mean'].to('cpu').view(-1, 1, 1)\n",
    "            std = self.stats['std'].to('cpu').view(-1, 1, 1)\n",
    "            img = (img - mean) / (std + 1e-8)\n",
    "            \n",
    "            batch.append(img)\n",
    "        \n",
    "        # Stack into batch and move to device\n",
    "        batch = torch.stack(batch).to(self.device)\n",
    "        \n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            features = self.encoder(batch)\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    \n",
    "    # # ... existing extract_features and batch_extract_features methods ...\n",
    "    \n",
    "    # def visualize_predictions(self, image, num_patches=4, mask_scale=(0.15, 0.2)):\n",
    "    #     \"\"\"Visualize predictions for a single image.\n",
    "\n",
    "    #     Args:\n",
    "    #         image: Input image in numpy format (C, H, W)\n",
    "    #         num_patches: Number of patches to visualize\n",
    "    #         mask_scale: Scale of the target blocks\n",
    "\n",
    "    #     Returns:\n",
    "    #         predictions: Tensor of predicted features\n",
    "    #     \"\"\"\n",
    "    #     # Extract features\n",
    "    #     features = self.extract_features(image)\n",
    "\n",
    "    #     # Prepare masks\n",
    "    #     masks = [torch.randint(0, 2, (features.size(0), num_patches)).to(self.device) for _ in range(num_patches)]\n",
    "\n",
    "    #     # Run predictor if available\n",
    "    #     if self.predictor is not None:\n",
    "    #         with torch.no_grad():\n",
    "    #             predictions = self.predictor(features, masks)\n",
    "    #         return predictions\n",
    "    #     else:\n",
    "    #         return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "34f4aacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your trained model checkpoint\n",
    "checkpoint_path = 'models/jepa-latest.pth.tar'\n",
    "stats_path = 'models/dataset_stats.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2402da91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample from zarr file\n",
    "zarr_path = 'data/ssl4eos12_val_seasonal_data_000001.zarr'\n",
    "zarr_data = zarr.open(zarr_path, mode='r')\n",
    "bands_data = zarr_data['bands'][:]  # bands data is of dimension (64, 4, 12/13, 264, 264)\n",
    "bands_data = np.array(bands_data) \n",
    "\n",
    "# combining the first two dimensions into one dimension\n",
    "bands_data = bands_data.reshape(-1, *bands_data.shape[2:])\n",
    "\n",
    "# retrieving a single sample\n",
    "sample = bands_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1f820a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "87cd0b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = get_inference_transforms(image_size=224)\n",
    "# # Apply the transform to the sample\n",
    "# sample = transform(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b780f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ecbbe558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marcus\\AppData\\Local\\Temp\\ipykernel_40556\\1330071897.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.stats = torch.load(stats_path, map_location=self.device)\n",
      "C:\\Users\\Marcus\\AppData\\Local\\Temp\\ipykernel_40556\\1330071897.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: torch.Size([1, 256, 384])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the feature extractor\n",
    "extractor = IJEPA_Extractor(checkpoint_path, stats_path, 'cpu')\n",
    "\n",
    "# Extract features\n",
    "features = extractor.extract_features(sample)\n",
    "print(f\"Feature shape: {features.shape}\")\n",
    "\n",
    "# These features can now be used for downstream tasks like:\n",
    "# - Classification with a linear head\n",
    "# - Clustering\n",
    "# - Similarity search\n",
    "# - Image retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b413550",
   "metadata": {},
   "source": [
    "## Linear Probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5e643c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, feature_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Linear(feature_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Average pooling over patch dimension to get global features\n",
    "        x = x.mean(dim=1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Create and train classifier\n",
    "feature_dim = 384  # For vit_small\n",
    "num_classes = 10   # Example: 10 land cover classes\n",
    "classifier = LinearClassifier(feature_dim, num_classes).to(device)\n",
    "\n",
    "# # Define optimizer\n",
    "# optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# # Train loop (pseudo-code)\n",
    "# for epoch in range(num_epochs):\n",
    "#     for images, labels in train_dataloader:\n",
    "#         # Extract features using frozen encoder\n",
    "#         features = extractor.batch_extract_features(images)\n",
    "        \n",
    "#         # Forward pass through classifier\n",
    "#         outputs = classifier(features)\n",
    "#         loss = criterion(outputs, labels)\n",
    "        \n",
    "#         # Backward and optimize\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18429384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(features).argmax()  # use model for class prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc93f8a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
